{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-03T11:37:19.902297Z",
     "iopub.status.busy": "2023-11-03T11:37:19.902024Z",
     "iopub.status.idle": "2023-11-03T11:37:32.828333Z",
     "shell.execute_reply": "2023-11-03T11:37:32.827250Z",
     "shell.execute_reply.started": "2023-11-03T11:37:19.902271Z"
    },
    "id": "03QWIa8UTB2f"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "!python -m pip install lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-03T11:37:32.830443Z",
     "iopub.status.busy": "2023-11-03T11:37:32.829807Z",
     "iopub.status.idle": "2023-11-03T11:37:45.235911Z",
     "shell.execute_reply": "2023-11-03T11:37:45.234824Z",
     "shell.execute_reply.started": "2023-11-03T11:37:32.830411Z"
    },
    "id": "ssMB6IcVTB2h",
    "outputId": "d4c3840f-3123-4e11-c44f-b987f089e6cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:37:45.237680Z",
     "iopub.status.busy": "2023-11-03T11:37:45.237357Z",
     "iopub.status.idle": "2023-11-03T11:37:45.242271Z",
     "shell.execute_reply": "2023-11-03T11:37:45.241265Z",
     "shell.execute_reply.started": "2023-11-03T11:37:45.237649Z"
    },
    "id": "EJdnDgWlTB2i"
   },
   "outputs": [],
   "source": [
    "# TODO: need to test with 1 minute resampled dataset\n",
    "# Main Idea has now been adapted to WaveNet - FPN1D + resnet replaced with WaveNetBlocks, and classifier replaced with GRU + FC\n",
    "# May need to check dataloader shapes and forward function shapes. Permutes may not be necessary.\n",
    "# Note in original wavenet approach had input channel 3, not 4 (This was done in preprocessing, could be due to some NN preference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:37:45.246259Z",
     "iopub.status.busy": "2023-11-03T11:37:45.246015Z",
     "iopub.status.idle": "2023-11-03T11:37:51.404402Z",
     "shell.execute_reply": "2023-11-03T11:37:51.403384Z",
     "shell.execute_reply.started": "2023-11-03T11:37:45.246237Z"
    },
    "id": "nllS_xQjTB2i"
   },
   "outputs": [],
   "source": [
    "# Data.py from Martin E's repo. Contains some data prep related code. Quite impoirtant to understand this.\n",
    "# Important also to understand data size for model modifications\n",
    "\n",
    "\"\"\"\n",
    "Time Encoding\n",
    "The function time_encoding is used to convert timestamps in a DataFrame to a numerical form that can be used by machine learning models.\n",
    "It does this by mapping each minute of the day to a sinusoidal wave value, likely with the intent to capture cyclical patterns in the data\n",
    "(such as circadian rhythms).\n",
    "\n",
    "\n",
    "Dataset Sampling:\n",
    "The CMITimeSeriesSampler class extends PyTorch's Dataset class.\n",
    "It is designed to handle time-series data for a sleep detection dataset by sampling a fixed-size segment (sample_size) from a longer time series.\n",
    "The sampling ensures that the segments are continuous in time by checking the difference in a step column.\n",
    "This approach is useful for handling large time-series datasets that are too large to process in full\n",
    "or when the model benefits from learning from shorter, contiguous segments.\n",
    "\n",
    "\n",
    "DataModule:\n",
    "The CMIDataModule class is a part of the PyTorch Lightning framework, which streamlines the process of organizing your data loading.\n",
    "It defines a setup method that prepares the data for the model by reading from a parquet file, normalizing a feature, encoding timestamps,\n",
    "and then creating a dataset object with the specified features and target.\n",
    "It further splits the dataset into training and validation sets.\n",
    "Data loaders for training and validation are also provided, which allow for easy batching and shuffling of the data for use in training loops.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "def time_encoding(ts_df: pd.DataFrame):\n",
    "    \"\"\"Use most common awake and onset times as a prior for encoding the timestamp to\n",
    "    a numerical value. Encoding is done at minute resolution\n",
    "\n",
    "    * It calculates two types of sinusoidal values based on the minute of the day .\n",
    "    * Maps these to a predefined circadian rhythm pattern using sinusoidal functions raised to the 24th power, which likely sharpens the curve.\n",
    "    * The timestamp is decomposed into minutes and these minutes are mapped to the pre-calculated sinusoidal values to create a continuous circadian pattern for dataset.\n",
    "    * Two new columns, onset prior and awake prior, created.\n",
    "    \"\"\"\n",
    "    n_mins_day = 60 * 24 # If we arent resampling with 1 min, we need to change this to the correct timescale of the dataset\n",
    "    awake_prior_vals = np.sin(np.linspace(0, np.pi, n_mins_day) + 0.208 * np.pi) ** 24\n",
    "    awake_prior_dict = dict(zip(range(1440), awake_prior_vals))\n",
    "    onset_prior_vals = np.sin(np.linspace(0, np.pi, n_mins_day) + 0.555 * np.pi) ** 24\n",
    "    onset_prior_dict = dict(zip(range(1440), onset_prior_vals))\n",
    "    time_df = pd.DataFrame()\n",
    "    time_df[\"onset_prior\"] = (\n",
    "        (ts_df.timestamp.dt.hour * 60 + ts_df.timestamp.dt.minute)\n",
    "        .map(onset_prior_dict)\n",
    "        .astype(np.float32)\n",
    "    )\n",
    "    time_df[\"awake_prior\"] = (\n",
    "        (ts_df.timestamp.dt.hour * 60 + ts_df.timestamp.dt.minute)\n",
    "        .map(awake_prior_dict)\n",
    "        .astype(np.float32)\n",
    "    )\n",
    "    return time_df\n",
    "\n",
    "\n",
    "class CMITimeSeriesSampler(Dataset):\n",
    "    \"\"\"Dataset class for sampling time-series from the CMI Sleep Detection dataset.\n",
    "    A random time-series of size 'sample_size' is sampled from the specified series\n",
    "    index\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, series_df, sample_size: int, feat_cols: list, target_col: str\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.feat_cols = feat_cols\n",
    "        self.target_col = target_col\n",
    "        self.series_df = series_df\n",
    "        self.series_grps = series_df.groupby(by=\"series_id\")\n",
    "        self.series_ids = list(self.series_grps.groups.keys())\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def check_timeseries_continuity(self, ts_df):\n",
    "        # Ensures that the sampled time series is continuous (i.e., there are no gaps in the data).\n",
    "        # Computes the difference between consecutive steps and checks if all differences are equal, implying continuity.\n",
    "        step_sizes = ts_df[\"step\"].diff()[1:].astype(int)\n",
    "        is_cont = (step_sizes == step_sizes.iloc[0]).all()\n",
    "        return is_cont\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.series_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #  Retrieves a continuous sample of the specified size from the dataset.\n",
    "        # Finds a random start index that allows for a contiguous segment of the desired sample_size.\n",
    "        # Samples the data, ensuring it is continuous.\n",
    "        # Extracts the features and target values for the sampled segment and returns them as a tuple.\n",
    "        sid = self.series_ids[index]\n",
    "        sample_df = self.series_grps.get_group(sid)\n",
    "        sample_df = sample_df.reset_index(drop=True)\n",
    "        assert len(sample_df) > self.sample_size\n",
    "\n",
    "        is_cont_ts = False\n",
    "        while ~is_cont_ts:\n",
    "            start_idx = np.random.randint(0, len(sample_df) - self.sample_size)\n",
    "            end_idx = start_idx + self.sample_size\n",
    "            ts_df = sample_df[start_idx:end_idx]\n",
    "            is_cont_ts = self.check_timeseries_continuity(ts_df)\n",
    "        #print(ts_df.columns)\n",
    "        assert all(col in ts_df for col in [\"anglez_1min_mean\", \"enmo_1min_mean\", \"onset_prior\", \"awake_prior\"]), \"Required columns are missing in the dataframe!\"\n",
    "        X_data = ts_df[self.feat_cols].T.to_numpy()\n",
    "        y_data = ts_df[self.target_col].to_numpy().astype(np.float32)\n",
    "\n",
    "        return X_data, y_data\n",
    "\n",
    "\n",
    "class CMIDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, datapath: str, batch_size: int, sample_size: int):\n",
    "        super().__init__()\n",
    "        self.datapath = datapath\n",
    "        self.batch_size = batch_size\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        #  Prepares the data for the model.\n",
    "        # Loads the dataset from the given path.\n",
    "        # Performs feature normalization and time encoding.\n",
    "        # Creates a PyTorch Dataset object using the preprocessed data.\n",
    "        # Splits the dataset into training and validation sets using a random split with a fixed seed for reproducibility.\n",
    "        series_df = pd.read_parquet(self.datapath)\n",
    "        series_df[\"anglez_1min_mean\"] = series_df[\"anglez_1min_mean\"] / 90.0\n",
    "        time_enc_df = time_encoding(series_df)\n",
    "        series_df = pd.concat([series_df, time_enc_df], axis=1)\n",
    "\n",
    "        feat_cols = [\"anglez_1min_mean\", \"enmo_1min_mean\", \"onset_prior\", \"awake_prior\"]\n",
    "        target_col = \"asleep\"\n",
    "        dset = CMITimeSeriesSampler(\n",
    "            series_df,\n",
    "            sample_size=self.sample_size,\n",
    "            feat_cols=feat_cols,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        self.train_dset, self.val_dset = random_split(\n",
    "            dset, [0.8, 0.2], generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Creates a DataLoader for the training set.\n",
    "        return DataLoader(self.train_dset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Creates a DataLoader for the validation set.\n",
    "        return DataLoader(self.val_dset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:54:11.966443Z",
     "iopub.status.busy": "2023-11-03T11:54:11.966035Z",
     "iopub.status.idle": "2023-11-03T11:54:12.018418Z",
     "shell.execute_reply": "2023-11-03T11:54:12.017397Z",
     "shell.execute_reply.started": "2023-11-03T11:54:11.966402Z"
    },
    "id": "hWi1hEKETB2k"
   },
   "outputs": [],
   "source": [
    "# Model.py from Martin E's repo. Contains the model structure. This should be replaced for WaveNet Adaptation. Need to understand the data format and interface.\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "\n",
    "##################################################################### Following is WaveNet-LSTM APproach\n",
    "# TODO is to replace FPN1D + ResNetBlocks with these\n",
    "# ResNetBlocks have the same input and output size\n",
    "class Wave_Block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dilation_rates, kernel_size):\n",
    "        super(Wave_Block, self).__init__()\n",
    "        self.num_rates = dilation_rates\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "\n",
    "        self.convs.append(nn.Conv1d(in_channels, out_channels, kernel_size=1))\n",
    "        dilation_rates = [2 ** i for i in range(dilation_rates)]\n",
    "        for dilation_rate in dilation_rates:\n",
    "            self.filter_convs.append(\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate))\n",
    "            self.gate_convs.append(\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate))\n",
    "            self.convs.append(nn.Conv1d(out_channels, out_channels, kernel_size=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs[0](x)\n",
    "        res = x\n",
    "        for i in range(self.num_rates):\n",
    "            x = torch.tanh(self.filter_convs[i](x)) * torch.sigmoid(self.gate_convs[i](x))\n",
    "            x = self.convs[i + 1](x)\n",
    "            res = res + x\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead_WaveNet(nn.Module):\n",
    "    def __init__(self, in_chs) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_chs, 1) # Modified Linear Layer to be 1 instead of original 3.\n",
    "        #self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Regarding squeezing, essentially it is to conver the data into 1D format\n",
    "    x = self.conv1(x).squeeze(): If the kernel size for conv1 equals the length of the input, this convolution would produce an output with a single time-step (length 1). Squeezing this result would remove the redundant dimension, leaving a 2D tensor\n",
    "    y_logits = self.conv2(x).squeeze(): Since conv2 reduces the channel dimension to 1, the output of conv2 would have the shape [batch_size, 1, length]. Squeezing it would remove the singleton channel dimension, leaving a 2D tensor [batch_size, length]\n",
    "\n",
    "    # We could replace the approach with a fully connected layer, as shown below for conv2\n",
    "    class ClassificationHead(nn.Module):\n",
    "    def __init__(self, in_chs, kernel_size, num_features) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_chs, in_chs // 4, kernel_size=kernel_size, padding=\"same\"\n",
    "        )\n",
    "        # Assuming that the convolutional layers do not change the length of the input,\n",
    "        # we can calculate the number of features as in_chs//4 * length_of_sequence\n",
    "        self.fc = nn.Linear(in_chs // 4 * num_features, 1)  # num_features is the length of the sequence\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        x = F.relu(self.conv1(x))  # Typically, we apply a non-linearity after the convolution\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n",
    "        y_logits = self.fc(x).squeeze()  # Fully connected layer\n",
    "        y_proba = torch.sigmoid(y_logits)  # Apply sigmoid to get probabilities\n",
    "        return y_logits, y_proba\n",
    "    TLDR, we are able to replace the conv layers with FC1 layers that should be faster to work with, LSTM-compatible, and and in alignment with original wavenet approach.\n",
    "    \"\"\"\n",
    "    def forward(\n",
    "        self, x: torch.FloatTensor\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        #x = self.fc(x)\n",
    "        #x = self.fc2(x)\n",
    "        y_logits = self.fc(x).squeeze()\n",
    "        y_proba = F.sigmoid(y_logits)\n",
    "        return y_logits, y_proba\n",
    "\n",
    "\n",
    "\n",
    "class EventDetectionCNN(nn.Module):\n",
    "    def __init__(self, in_chs: int, kernel_size=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Input layer consits of the input size\n",
    "        # The output channel for Conv1D is feat_chs, but due to concatenation of all 3 steps and batchnorm,\n",
    "        # The input for resnet blocks is hence resnetblock1D\n",
    "\n",
    "       #Also replacing FPN1D with waveblocks, as they already have dilution made available\n",
    "        \"\"\"\n",
    "        #in_chs =4, feat_chs =32\n",
    "        self.input_layer = FPN1D(in_chs, feat_chs, kernel_size=31)\n",
    "        \"\"\"\n",
    "\n",
    "        #Aim to replace the following original code with wavenet\n",
    "        \"\"\"\n",
    "        # Resnetblock input and output is the same size.\n",
    "        #Original code\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *[ResNetBlock1D(resnet_feat_chs, kernel_size=11)] * n_resnet_blocks\n",
    "        )\n",
    "        \"\"\"\n",
    "        #In_chs = 4. This is the same as in original FPN1D\n",
    "        self.wave_block1 = Wave_Block(in_chs, 32, 8, kernel_size)\n",
    "        self.wave_block2 = Wave_Block(32, 64, 4, kernel_size)\n",
    "        self.wave_block3 = Wave_Block(64, 128, 1, kernel_size)\n",
    "\n",
    "        self.LSTM = nn.GRU(input_size=128, hidden_size=128, num_layers=4,\n",
    "                           batch_first=True, bidirectional=True)\n",
    "\n",
    "\n",
    "\n",
    "        self.cls_head = ClassificationHead_WaveNet(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() # Convert to float32 tensor\n",
    "        #print(\"shape of input\")\n",
    "        #x = x.permute(0, 2, 1)  # Permute needed for shape. Note, May not be exactly what we want for this project\n",
    "        x = self.wave_block1(x)\n",
    "        x = self.wave_block2(x)\n",
    "        x = self.wave_block3(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, h = self.LSTM(x) # Permute needed for shape. Note, May not be exactly what we want for this project\n",
    "        y_logits, y_proba = self.cls_head(x)\n",
    "        return y_logits, y_proba\n",
    "\n",
    "    def predict_from_df(self, df, feat_cols):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(df[feat_cols].T.to_numpy()).float().unsqueeze(0)\n",
    "            _, y_proba = self.forward(x)\n",
    "        return y_proba.numpy()\n",
    "\n",
    "# This is the main model module that gets called.\n",
    "# Modified for WaveNet\n",
    "class CMISleepDetectionWaveNetCNN(pl.LightningModule):\n",
    "    # in_chs =4\n",
    "    def __init__(self, in_chs: int):\n",
    "        super().__init__()\n",
    "        self.model = EventDetectionCNN(in_chs)\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_in, y_target = batch\n",
    "        y_logits, _ = self(x_in)\n",
    "        loss = self.bce_loss(y_logits, y_target)\n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_in, y_target = batch\n",
    "        y_logits, y_proba = self(x_in)\n",
    "        val_loss = self.bce_loss(y_logits, y_target)\n",
    "        self.log(\"val/loss\", val_loss)\n",
    "\n",
    "        # calculate accuracy\n",
    "        y_pred = y_proba > 0.5\n",
    "        y_target = y_target > 0.5\n",
    "        val_acc = torch.sum(y_pred == y_target) / y_target.nelement()\n",
    "        self.log(\"val/acc\", val_acc)\n",
    "\n",
    "        return val_loss, val_acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:37:54.947267Z",
     "iopub.status.busy": "2023-11-03T11:37:54.946924Z",
     "iopub.status.idle": "2023-11-03T11:37:54.960269Z",
     "shell.execute_reply": "2023-11-03T11:37:54.959447Z",
     "shell.execute_reply.started": "2023-11-03T11:37:54.947235Z"
    },
    "id": "ACpB8OKkTB2l"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:37:54.961758Z",
     "iopub.status.busy": "2023-11-03T11:37:54.961488Z",
     "iopub.status.idle": "2023-11-03T11:37:54.972283Z",
     "shell.execute_reply": "2023-11-03T11:37:54.971435Z",
     "shell.execute_reply.started": "2023-11-03T11:37:54.961734Z"
    },
    "id": "DkbSYX0rTB2m"
   },
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/PyTorch_Lightning\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datapath = \"/kaggle/input/cmi-train-1min-resampled/train_series_1min.parquet\"\n",
    "datamodule = CMIDataModule(datapath, batch_size=128, sample_size=60 * 12)\n",
    "#input channel is 4, because we have 4 feature cols [\"anglez_1min_mean\", \"enmo_1min_mean\", \"onset_prior\", \"awake_prior\"]\n",
    "# Original wavenet has 3 channels. Created by rolling window over a single wav file. Maybe use it.\n",
    "\n",
    "#Input size: torch.Size([128, 4, 720])\n",
    "#Target size: torch.Size([128, 720])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:53:55.491576Z",
     "iopub.status.busy": "2023-11-03T11:53:55.490650Z",
     "iopub.status.idle": "2023-11-03T11:54:11.963789Z",
     "shell.execute_reply": "2023-11-03T11:54:11.962615Z",
     "shell.execute_reply.started": "2023-11-03T11:53:55.491539Z"
    },
    "id": "_pEFGJ-3TB2m"
   },
   "outputs": [],
   "source": [
    "model = CMISleepDetectionWaveNetCNN(in_chs=4)\n",
    "\n",
    "model_ckpt_callback = ModelCheckpoint(save_top_k=1, monitor=\"val/acc\", mode=\"max\")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val/acc\", mode=\"max\", patience=10)\n",
    "trainer = pl.Trainer(\n",
    "    log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=5,\n",
    "    max_epochs=100,\n",
    "    accelerator=\"cuda\",\n",
    "    callbacks=[early_stop_callback, model_ckpt_callback],\n",
    ")\n",
    "trainer.fit(model, datamodule)# Train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:40:24.280224Z",
     "iopub.status.busy": "2023-11-03T11:40:24.279917Z",
     "iopub.status.idle": "2023-11-03T11:40:24.290271Z",
     "shell.execute_reply": "2023-11-03T11:40:24.289508Z",
     "shell.execute_reply.started": "2023-11-03T11:40:24.280197Z"
    },
    "id": "JH1mwJ8VTB2n"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:40:24.291749Z",
     "iopub.status.busy": "2023-11-03T11:40:24.291413Z",
     "iopub.status.idle": "2023-11-03T11:40:29.311826Z",
     "shell.execute_reply": "2023-11-03T11:40:29.310871Z",
     "shell.execute_reply.started": "2023-11-03T11:40:24.291715Z"
    },
    "id": "txEUWoA-TB2o"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir /kaggle/working/lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_6qLdwRTB2p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
